---
layout : single
title : "[RAG] 02. Langchain 랭체인밋업 Q2"
categories : [RAG, LLM]
tag : [Modular RAG, LangGraph]
---





해당 게시글은 테디노트 TeddyNote [영상](https://www.youtube.com/watch?v=4JdzuB702wI&ab_channel=%ED%85%8C%EB%94%94%EB%85%B8%ED%8A%B8TeddyNote)을 정리한 내용입니다.



## 개요

### LangGraph 탄생 배경

1. LLM 생성 답변의 Hallucination 가능성
2. RAG 기반 답변의 "사전지식" 기반 답변 가능성
3. 문서 검색에서 원하는 내용이 없을 경우
   * "인터넷" 혹은 "논문"에서 부족한 정보를 검색하여 지식을 보강 방법 강구



### Conventional RAG 문제점

* 사전에 정의된 데ㅣ터 소싱(PDF, DB, Table) 자원
* 사전에 정의된 Fixed Size Chunk
* 사전에 정의된 Query 입력
* 사전에 정의된 검색 방법
* 신뢰하기 어려운 LLM 혹은 Agent
* 고정된 프롬프트 형식
* LLM의 답변 결과에 대한 문서와의 관련성/신뢰성



"Document Loader(데이터로드) -> Answer(답변)" RAG 파이프라인이 단방향 구조이기 때문

* 모든 단계를 한 번에 다 잘해야 함
* 이전 단계로 되돌아가기 어려움
  * 이전 과정의 결과물을 수정 불가



###  LangGraph 제안

Rag 파이프라인을 보다 유연하게 설계

* 각 세부과정을 노드(Node)라고 정의
* 이전 노드 > 다음 노드: 엣지(Edge) 연결
  * 엣지를 순방향이 아닌 역방향으로 진행될 수 있음
  * 예를 들면, retrieve된 문서가 질문과 답변이 없을 경우, 다시 검색하도록 생성할 수 있음
* 조건부 엣지를 통해 분기 처리





## 개념 및 코드

### LangGraph

* Node(노드), Edge(엣지), State(상태관리)를 통해 LLM을 활용한 워크플로우에 순환(Cycle) 연산 기능을 구현하여 쉽게 흐름을 제어.
* RAG파이프라인의 세부 단계별 흐름제어가 가능
* Conditional Edge: 조건부(if, elif, else와 같은..) 흐름 제어
* Human-in-the-loop: 필요시 중간 개입하여 다음 단계를 결정
* Checkpointer: 과거 실행 과정에 대한 "수정" & "리플레이" 가능
  * Memory 기능과 유사
  * Snap shot과 유사하게 과거의 상태로 돌아가거나, 현재 몇 번 반복되고 있는지 확인 가능



**<u>주요용어</u>**

* Node(노드): 어떤 작업(task)을 수행할지 정의
  * runable 객체가 정확한 표현
* Edge(엣지): 다음으로 실행할 동작 정의
* State(상태): 현재의 상태 값을 저장 및 전달하는데 활용
* Conditional Edge(조건부 엣지): 조건에 따라 분기 처리



### 상태(State)

노드와 도드 간에 정보를 전달할 때 상태(State) 객체에 담아 전달

* TypedDict: 일반 파이썬 dict에 타입힌팅을 추가한 개념이지만, 쉽게 dictionary로 생각해도 됨
* 모든 값을 다 채우지 않아도 됨
* 새로운 노드에서 값을 덮어쓰기(Overwrite) 방식으로 채움

```python
from typing import TypedDict

class GraphState(TypedDict):
    goal: str
    todo: list[str]
    current_job: str
    total_time: int
    time_spent: int
    status: str
```

* 각 노드에서 새롭게 업데이터 하는 값은 기존 Key 값을 덮어쓰는 방식
* 노드에서 필요한 생태 값을 조회하여 동작에 활용할 수 있음



### 상태(State) - RAG 사례

* 질문을 재작성 요청
  * QueryTransform: 질문 재작성
* 문서 검색을 재요청
  * Context Retrieval조정(Chunk, 다른 검색기, web search, ... )
* 답변을 재생성 요청
  * 프롬프트 조정 & 다른 LLM 사용(GPT, Claude, Local)



### 노드(Node)

* 함수로 정의
* 입력인자: 상태(State) 객체
* 반환(return)
  * 대부분 상태(State) 객체
  * Conditional Edge의 경우 다를 수 있음

```python
def retrieve_document(state: GraphState) -> GraphState:
    retrieved_docs = pdf_retriever.invoke(state["question"])
    return GraphState(context=format_docs(retrieved_docs))
```



### Graph 생성 후 노드 추가

* 이전에 정의한 함수를 Graph에 추가
* add_node("노드이름", 함수)

```python
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver


workflow = StateGraph(GarphState)

# 노드 추가
workflow.add_node("retrieve", retrieve_document)
workflow.add_node("llm_answer", llm_answer)

# 노드 함수
def retrieve_document(state: GraphState) -> GraphState:
	retrieved_docs = pdf_retriever.invoke(state["question"])
    return GraphState(context=format_docs(retrieved_docs))

def llm_answer(state: GraphState) -> GraphState:
    return GraphState(answer_chain.invoke({"question": sstate["question"], "context": state["context"]}))
```



### 엣지(Edge)

* 노드에서 노드간의 연결
* add_edge("노드이름", "노드이름")
  * from -> to

```python
workflow.add_edge("retrieve", "llm_answer")
workflow.add_edge("llm_answer", "relevance_check")
```





### 조건부 엣지(Conditional Edge)

* 노드에 조건부 엣지를 추가하여 분기를 수행 가능
* add_conditional_edges("노드이름", 조건부 판단 함수, dict로 다음 단계 결정)

```python
workflow.add_conditional_edges(
	"relevance_check",
	is_relevant,
	{
        "grounded": END,
        "notGrounded": "llm_answer",
        "notSure": "llm_answer"
    }
)
```

* "relevance_check" 노드에서 나온 결과를 is_relevant 노드에 입력
* 반환된 값은 "grounded", "notGrounded", "notSure" 중 하나
  * value에 해당하는 값이 END일 경우, Graph 실행 종료
  * "llm_answer"와 같이 노드이름이면 해당 노드로 연결



### 시작점 지정

* set_entry_point("노드이름")
* wlwjdgks tlwkrwjaqnxj ㅎㄱ메ㅗrk tlwkr

```python
workflow.set_entry_point("retrieve")
```



### 체크포인터(memory)

* Checkpointer: 각 노드간 실행결과를 추적하기 위한 메모리(대화에 대한 기록과 유사 개념)
* 체크포인터를 활용하여 특정 시점(Snapshot)으로 되돌리기 기능 가능
* compile(checkpointer = memory)지정하여 그래프 생성

```python
memory = MemorySaver()
app = workflow.compile(checkpointer = memory)
```



### 그래프 시각화

* get_graph(xray=True).drqw_mermaid_png()
  * 생성한 그래프 시각화

```python
display(
	Image(app.get_graph(xray=True).draw_mermaid_png())
)
```



### 그래프 실행

* RunnableConfig
  * recursion_limit: 최대 노드 실행 개수를 지정합니다.(13인 경우: 총 13개의 노드까지 실행합니다)
  * thread_id: 그래프 실행 아이디를 기록하고, 추후 추적하기 위한 목적으로 활용합니다.
* 상태(State)로 시작합니다.
  * 여기서 "question"에 질문만 입력하고 상태를 첫 번째 노드에게 전달 합니다.
* invoke(상태, config) 전달하여 실행합니다.

```python
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(recursion_limit=13, configurable={"thread_id": "SELF-RAG"})

inputs = GraphState(question="삼성전자가 개발한 생성형 AI의 이름은?")
output = app.invoke(inpus, config=config)

print("Question: \t": output["question"])
print("Answer: \t": output["answer"])
print("Relevance: \t": output["relevance"])
```

