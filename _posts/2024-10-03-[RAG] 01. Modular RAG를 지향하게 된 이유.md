---
layout : single
title : "[RAG] 01. Modular RAG를 지향하게 된 이유"
categories : [RAG, LLM]
tag : [Modular RAG, Advanced RAG]
---





해당 게시글은 AI프렌즈에 [영상](https://www.youtube.com/watch?v=aMUopbBrAmA&t=1224s&ab_channel=AI%ED%94%84%EB%A0%8C%EC%A6%88)을 개인적으로 정리한 내용입니다.



## RAG 개요

### RAG Background

1. 할루시네이션
2. 최신 정보의 반영 불가 문제
   * 일반 LLM은 지식의 cut-off를 가짐(ex. 해당 모델은 2023년 11월 까지 데이터를 학습한 모델)
3. 도메인 특화
4. 지식의 불분명한 출처



### RAG의 패러다임 변화

* Naive RAG - Advanced RAG - Modular RAG
  * 패러다임은 End-to-End RAG 파이프라인의 변화에 따라 나뉨
* Naive RAG > Advance RAG
  * 더 나은 RAG 성능 개선에 초점을 맞춤
  * Pre-Retrival, Post-Petrieval 등이 Naive RAG에서 추가 됨
* Advanced RAG > Modular RAG
  * RAG를 프로덕션 레벨에서의 유지보수, 효율적인 설계를 고려
  * RRR, DSP, ITER-RETGEN, SELF-RAG 등의 패턴이 존재



## Naive RAG

### Naive RAG

* 사전단계

  * Indexing : PDF, Word, Markdown 등에서 텍스트 데이터 추출, 이미지 데이터 추출

  * Chunking: 작은 단위로 분할

  * Embedding: Vector로 인코딩

  * Database: 임베딩된 Vector 저장

* 실행 단계

  * Retrive: Database에서 질문(Query)에 답변하기 위한 정보 검색
  * Generation: 검색된 정보를 문맥(Context)에 추가하여 답변 생성



### Naive RAG의 한계

* 쿼리에 대한 얕은 이해
  * 쿼리와 문서 Chunk 사이의 의미론적 유사성이 항상 일치하는 것은 아님
  * 검색을 위해 유사도 계산에만 의존하는 것은 쿼리와 문서 간의 관계에 대한 심층적인 탐색이 부족
    * 한글로 "만나서 반가워"를 입력하고 Retrive가 필요한 Task에서 "Hi nice to meet you!"로 저장된 document는 의미론적으로 유사하지만 Retrive가 잘 안되는 경우가 있음
    * 한글로 입력하면 한글 문서라는 이유로 유사도가 높게 나타남
* 검색 중복 및 노이즈
  * 검색된 모든 Chunk를 LLM에 직접 공급하는 것이 항상 유익한 것은 아님
  * 연구에 따르면 중복되고 노이즈가 많은 정보는 LLM이 핵심 정보를 식별하는 데 방해가 되어 잘못된 응답을 생성(Hallucination)할 위험이 높아질 수 있음



## Advanced RAG

### Advanced RAG

* 이전의 Naive RAG가 가지고 있었던 한계를 극복하기 위한 다양한 고급 방법론
* Indexing
  * 계층적 구조의 Indexing
  * Semantic Chunking
* Pre-Retrieval
  * Query Rewrite, Query Expansion
  * Query Transformation
* Retrieval
  * Hybrid Search(키워드 검색 + 시맨틱 검색)
* Post-Retrieval
  * Reranker, Reorder



### Advanced RAG - Indexing

* Metadata
  * metadata에 연도, 출처(파일명, URL) 등을 추가
    * Self-Query Retriever: Query 문으로 데이터 필터링에 활용
    * ex) "2022년도에 작성한 문서 중에서 ~~~ 내용을 정리해줘"
  * Chunk 저장 시 아래의 메타정보를 추가함
    * 해당 Chunk의 문서 출처, Page, 문서 작성 일자 등
* Summary, Endity 등 추가 정보 생성 후 Indexing
  * 추상적인 질문에는 Summary/Entity 활용
    * 큰 맥락을 물어보는 경우, 큰 맥락의 정보가 저장되어 있는 DB에서 Retriver를 진행
    * Summary나 Entity DB를 검색
  * 구체적인 질문에는 작은 Chunk 활용
* Hierarchical Structure
  * 계층적 구조로 검색 범위를 좁히고, 대신 검색 Depth를 늘림
    * Multi-hop 질문의 경우 노드 간의 고나계를 활용하여 정확도 향상
    * Multi-hop Question이란, 답변을 얻기 위해 여러 단계의 추론이나 정보 조합이 필요한 복잡한 질문 유형
    * Graph DB를 많이 사용하고 있음
* Hybrid Indexing
  * Relational Database + Vector Database
    * RDB
      * 구조화된 데이터를 저장
      * 사용자의 질문 기반 필터링/대화 내용 저장
    * Vector DB
      * 문서 내용 기반 유사도 검색에 활용
  * 자연어로 질문이 들어오면, 기존 RDB에 적용할 수 있는 Query를 생성해서 검색
  * Vector DB 검색 결과와 Query 생성을 통해 조회된 결과를 동시에 사용하여 출력하는 구조



### Advanced RAG - Chunking Strategy

* Semantic Chunking
  * 의마상 유사한 단락을 기준으로 Chunking
  * 사용자들의 높은 평가를 받음
  * 다만, Chunking 과정에서 Clustering 등의 복잡한 절차를 진행하기에 많은 시간이 소모됨
* Small-to-Big
  * 자식 - 부모 Document 구조
  * 작은 Chunk 단위를 임베딩 한 뒤, Retrieval 단계에서 더 큰 Chunk 반환
    * ParentDocument Retriever
  * 부모 Documnt는 정의하기에 따라 다름
    * 해당 chunk가 포함된 페이지, 단락 등이 존재
  * 검색된 chunk의 앞뒤 context를 입력하여 더 좋은 답변을 생성하기 위함
* Sentence Window
  * 단일 문장 - 단일 문장 + 주변 문장을 저장
  * 고정된 수(예를 들어 2~3 문장) 의 주변 문장을 저장
  * 이후 단일 문장에 Top-N 결과에 포함되면 주변 문장이 포함된 더 큰 Chunk 를 반환



### Advanced RAG - (Pre) Retrieval

* Query Rewrite
  * Query의 의미를 보존하면서 모호성을 제거
  * 원래 Query의 의미를 유지하면서 더 명확하고 정보가 풍부한 형태로 Query를 재작성
  * Query Expansion 
  * 원래 쿼리 관련 용어나 동의어를 추가하여 검색 범위를 확장하는 기법
* Query Transformation
  * 쿼리의 구조나 형식을 변형
  * ㅇ예시)
    * 일반 질문을 SQL 쿼리문으로 변환
    * 일반 질문을 검색에 용이한 구문으로 변환



### Advanced RAG - Retrieval

* Hybrid Search
  * 키워드 검색 + 시맨틱 검색
    * 키워드 검색: 정확한 단어 매칭을 기반
    * 시맨틱 검색: 의미와 문맥을 이해하여 관련 정보 검색
    * 일반 사용자들은 키워드 기반의 검색에 익숙함으로, 키워드 검색에 가중치를 부여한 결과의 만족도가 높음
  * Hypothetical Question
    * 문서의 내용을 질문 형태로 변환함으로써, 사용자 쿼리와의 의미적 매칭을 향상
      * 각 도큐먼트에 맞는 가상의 질문을 생성
      * 생성된 가상의 질문을 임베딩
      * 사용자 쿼리와 가상 질문 임베딩 간의 유사도 계산
    * 질문을 기반으로 문서의 내용을 Retrieve하려고 했으나, 질문의 문장과 문서가 연결되지 않는 점을 해결하기 위해, 문장별 질문을 생성하고 생성 질문과 질문의 유사도를 계산함
  * HyDE(Hypothetical Document Embeddings)
    * 생성된 답변이 직접 쿼맂보다 임베딩 공간에 더 가깝다는 가정에 기초
      * 사용자 쿼리에 대한 가상의 답변 생성
      * 생성된 답변을 임베딩
      * 답변과 답변 강의 임베딩 유사성을 강조



### Advanced RAG - (Post) Retrival

* Reranker
  * 각 Query - Document 쌍의 관련성을 평가
  * Retriever가 대규모 문서 집합에서 빠르게 후보를 추출하는 데 초점을 맞추는 반면, Reranker는 이미 추출된 소수의 후보에 대해 더 정교한 분석을 수행
  * Retriever와 Reranker를 결합한 Two Stage Retrieval pipeline 주를 이룸
  * Reranker의 Embedding 속도가 매우 느려 Retrieval에 사용하는데 부적합함
* Context Reorder
  * Lost in the Middle 논문
  * LLM은 입력 텍스트의 초반부와 후반부에 있는 정보를 더 잘 활용함
  * 덜 관련된 문서는 목록의 중간에 배치하고, 관련성이 높은 문서는 시작과 끝에 배치
* Compressor(추가적인 조사 필요함)
  * 관련성이 낮은 정보를 제거하여 LLM에 입력되는 컨텍스트의 품질을 향상
  * Context Precision: 검색된 정보 중에서 실제로 관련 있는 정보의 비율
    * 할루시네이션 발생 가능성을 줄이는 데 도움
    * 토큰 비용 효율화(답변의 품질을 유지하면서 입력 크기를 줄임)
    * 처리속도 향상
  * 주요 방법론
    * LLMChainExtractor: 쿼리와 관련된 컨텍스트만 추출
    * Lexical-based Compression
      * LLMLingua
      * 문맥(Context)에서 중요한 토큰을 선별한 뒤, 쿼리 기반 토큰 Classifier가 중요 토큰 선별
    * Embedding-based Compression
      * 임베딩을 사용하여 관련성 높은 정보 필터
    * COCOM(COntext COmpression Model)
      * COmpressor Model을 파인튜닝하여 압축에 활용



### Advaned RAG 한계

* Advanced RAG의 실용성이 개선되었음에도 불구, 기능과 실제 어플리케이션 요구사항 사이에는 여전히 격차가 존재
* 프로덕션(Production)의 어려움
  * RAG 기술이 발전함에 따라 사용자의 기대치가 높아짐
  * 요구사항이 계속 징화, 어플리케이션이 더욱 복잡
  * 다양한 형태의 데이터 통합 필요
  * 파이프라인의 제어 및 유지보수의 어려움 존재



### Linear Structured RAG 한계

* 단방향 구조의 RAG(Naive RAG, Advanced RAG)
* Document Loader(데이터로드) > Answer(답변)
  * 모든 단계를 한 번에 다 잘해야 함
  * 이전 단계로 되돌아가기 어려움
    * 이전 과정의 결과물을 수정하기 어려움



## Modular RAG

### Modular RAG 개요

* LEGO와 같이 재구성 용이하고 보다 유연한 흐름을 만들 수 있는 프레임워크
* 논문: Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks
  * 독립적(Independant) - 기능 위주로 세분화
  * 유연하고 확정성(flexable & scalable) - 모듈의 추가 및 변경이 쉬움
  * 동적(dynamic) - 그래프 형식의 흐름 구성, 상황에 따른 분기 처리



### Modular RAG - 모듈

* 모듈, 하위 모듈 및 운영자로 구성된 아키텍처 설계를 통해 통합되고 구조화된 방식으로 RAG 시스템
* 각 단계(Step)를 모듈(Module) 별로 정의
  * 각 모듈은 하위 모듈(Sub Module)로 구성
* 하위 모듈은 플러그인 형식의 독립된 구조
  * 독립된 구조: 각 모듈은 조립형으로 설계
  * 병렬 구조: 여러 하위 무듈을 동시실행 후 병합 가능
  * 분기 구조: 상황에 따른 분기 처리가 가능



### Modular RAG - 독립적인 모듈 구성

* 동일한 형태의 입력과 출력을 반환하는 Interface
  * LEGO 방식의 조립이 가능한 Module 설계
  * 내부 구현은 모듈 별 목적에 따라 다르게 구현



### Modular RAG - 조립형 모듈

* 마치 LEGO 블록처럼 세부 모듈(Sub-Module)을 연결하여 구성
  * 모듈의 추가/변경 용이
* 독립적인 모듈의 장점을 활용
  * 각 Sub-Module의 교체가 쉬움
  * 단계의 전/후에 Sub-Module을 추가



### Modular RAG - 그래프 형식의 동적 흐름 구성

* 링크(https://github.com/hymie122/RAG-Survey)에서 Modular RAG 패턴들에 대한 주요 논문 확인 가능

* 평가자 > "문서 검색"에 대한 평가(Score) - 문서에 Query에 대한 답변을 위한 정보가 충분한지 판단
  * Success: 답변 생성
  * Fail
    * 검색된 문서의 정보가 불충분한 경우
      * 문서 검색을 위한 Query Rewrite
    * 질문이 모호한 경우
      * 보다 명확한 질문 or Query Decomposition을 통한 세부 질문으로 구성 후 검색
* 질문에 문서 검색이 필요한지 판단하여 Routing
  * Yes: 문서 검색 > 답변 생성
  * No
    * 즉각 답변 생성: 토큰 비용의 절약
    * 예시> "대한민국의 수도는 어디입니까?" > 즉각 답변
* 평가자 2는  RAG 답변에 대한 평가
* 평가자 관련 부분
  * LLM as a Judge
    * LLM에게 프롬프팅 방식으로 평가 기준을 입력
    * LLM이 평가 기준으로 답변을 평가



### Modular RAG 패턴 - Linear Pattern

* 선형 흐름 패턴은 가장 단순하고 가장 일반적으로 사용되는 패턴
* RAG 흐름 패턴은 주로 검색 전처리 - 검색 - 후처리 및 생성 모듈로 이루어진 구성
* Pre-Retrieve
  * Query Transform
    * Rewrite Query, HyDE
* Post-Retrieve
  * Reranker, Reorder
* RRR
  * Rewrite - Retrieve - Read



### Modular RAG 패턴 - Conditional Pattern(Routing)

* Query 입력에 따라 RAG vkdlvmfkdls tjsxor
* 라우팅 모듈이 존재
  * 예시) 입력된 Query에 따른 검색기 선택



### Modular RAG 패턴 - Branching Pattern

* Pre-Retrieve
  * 검색 소스 / 검색 프로세스 / 프롬프트 / 모델 등이 달라짐
  * 각 브랜치는 검색 & 생성을 개별적으로 수행한 다음 결과를 앙상블
  * 예시) Query 3개 생성 - 검색 - 생성 - 결과
* Generation
  * 모델이 동일한 문맥(Context)으로 각각의 답변을 도출한 다음 결과를 앙상블
  * 예시) GPT-4o, Claude 3.5 Sonnet, Llama3.1



### Modular RAG 패턴 - Loop Pattern

* Iterative N Pattern
  * 반복적인 검색 - 생성 과정을 토앻 복잡한 질문에 대한 답변의 품질을 향상
  * 동작방식
    * 검색과 생성 단계를 여러 번 반복
    * 미리 정해진 최대 반복 횟수(N)까지 프로세스를 반복
    * 각 반복마다 이전 출력을 활용하여 더 관련성 높은 정보를 검색하고 답변을 개선
*   ITER-RETGEN(https://arxiv.org/pdf/2305.15294)
  * Generation Augmented Retrieval(GAR)
    * 검색된 문석 기반 LLM 답변 생성
    * 생성된 답변을 Query와 결합하여 검색 수행
    * 새롭게 검색된 문서 기반으로 LLM 답변 생성
    * N회 반복 후 최종 답변 두출
* Tree of Clarification(https://arxiv.org/pdf/2310.14696)
  * 모호한 질문을 구체화 해나가는 과정
    * 각 노드에서 LLM은 해당 질문에 대한 답변 생성(w/ Ambiguous Question)
    * Question Clarfication을 위한 하위 노드 추가 생성(Disambiguous Question)
    * 관련 없는 노드 pruning / 관련성 있는 정보만 남김
    * 생성한 정보를 종합하여 Long Form 답변 생성
* Adaptive Retrieval Pattern
  * Retrieval 전 단계에서 Retrieval 과정이 필요한지를 판단
  * 동적방식: 검색(Retrieval)이 필요한 질문인지 판단
    * 검색이 필요하지 않은 경우: 즉각 답변 생성
    * 검색이 필요한 경우: ITER-N 혹은 REcursive Retrival Pattern 실행
* Forward-Looking Active Retrieval(FLARE)
  * 답변을 생성함에 있어 Retrival이 필요한지 판단
  * 답변에 대한 신뢰도(COnfidence)를 측정하여 동적으로 Query 구성
  * 동작방식
    * 쿼리 입력
    * 모델이 토큰을 생성하며 다음 문장을 반복적으로 예측
    * 모델이 생성한 답변이 Confidence가 낮다면, 생성된 문장을 Query로 문서 재검색
    * 재검색한 문서 + 생성한 문장을 입력으로 새로운 답변 생성
    * 응답이 완성될 떄까지 쿼리 입력 후 단계를 반복 수행



### Modular Rag 패턴 - Adaptive Pattern

* 응용한 Self-Reflective RAG(https://arxiv.org/pdf/2310.11511)

  * Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection을 응용한 버전
  * Self-RAG 논문에서는 reflection 토큰으로 retrieval하는 시기를 판단 > LLM as a Judge로 대체
  * 동작방식
    * 문서 Retrieval 후 관련성 평가
    * 관련성이 있다면 답변 생성, 관련성이 없다면 Query Rewrite후 Step 1 진행
    * 답변에 대한 Hallucination 평가
    * Hallucination 판별시 답변 재생성, 없다면 Relevance 체크
    * Relevance Check 통과시 최종 답변, 실패시 Query Rewrite 후 step1 진행

  

## LangGraph

### LangGraph 개요

* 워크플로우(Workflow)를 만드는 데 사용
* 상태 저장, 멀티 액터(Multi-Actor) 애플리케이션을 구축에 용이한 라이브러리
* 핵심기능
  * Cycle & Braching
    * 루프 및 조건문을 구현
  * Persistance
    * 그래프의 각 단계 후에 자동으로 상태를 저장
    * 노드 - 노드 간 상태 값을 전달
  * Low Level Control
    * LangGraph는 LangChain을 개발한 LangChain Inc 에서 구축했지만,  LangChain 없이도 사용 가능
* 주요 구성 요소
  * Node(노드)
    * Sub-Module: RAG 안의 세부 기능을 정의
  * Edge(엣지)
    * 노드 - 노드간 연결을 정의
    * 1:N, N:1 연결도 지워(병렬처리)
  * Conditional Edge(조건부 엣지)
    * 조건에 따라 분기 처리
  * State(상태)
    * 현재의 상태 값을 저장 및 전달하는데 활용
    * 상태 값은 노드의 결과값



### LangGraph 구현 예시

* Node 정의(split-pdf_node, ...)
* Node - Node 간 Edge로 연결하여 그래프 구조화
  * 병렬 처리 후 합산 가능
* 그래프를 출력하여 로직을 쉽게 확인 가능
* LangGraph Studio
  * Human in the Loop(사람의 개입)
  * 동적 메시지
  * 각 노드의 결과물을 실시간 출력