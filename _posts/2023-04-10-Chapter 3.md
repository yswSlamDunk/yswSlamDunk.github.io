---
layout : single
title : "[DeepLearningBasic] Chapter 3. 왜 우리는 인공 신경망을 공부해야 하는가?"
categories : [DeepLearning, DeepLearningBasic]
tag : [혁팬하임]
---

# 1. weight와 bias의 직관적 이해, 인공 신경망은 함수

* 인공신경

  * weight(중요도)를 곱하고 bias(민감도)와 함께 더하고, 액티베이션 함수에 통과

  * 주어진 입력에 대한 원하는 출력이 나오도록 weight와 bias를 학습시켜야 함


* 인공신경망

  * 인공신경망은 함수로, 주어진 입력에 대해 원하는 출력이 나오도록 하는 함수를 알아내는 것

  * 적절한 weight와 bias를 알아내는 것이 딥러닝의 목표




# 2. 선형 회귀

* 입력과 출력 간의 관계(함수)를 선형으로 놓고 추정하는 것
* 처음보는 입력에 대해서도 적절한 출력을 얻기 위함
* Loss(= cost)를 최소화하는 최적의 중요도와 민감도를 찾아야 함
  * 이때, error의 합을 단순히 구할 경우, 문제가 됨
  * error의 값이 음수와 양수가 존재하기 때문에 합이 0이 될 수 있음
  * 그렇기에, error 제곱의 합이 최소화 되도록 Loss를 설정해야 함
  * 절대값과 제곱 방안 모두 사용해도 되는데, 제곱을 더 많이 사용함
    * 절대값과 제곱을 그래프로 그렸을 때, Loss가 최소가 되는 점에서 멀어질수록 제곱이 더 민감하게 반응하기 때문
    * 절대값을 사용했을 경우, 미분이 안되는 문제가 있으나, 해당 문제는 꺾이는 점의 미분은 0이다 라는 예외를 주면 모든 경우에서 미분이 가능하기 때문에, 적절하지 못함
    * 다양한 이유에서 MSE(제곱) Loss를 사용함
* Loss는 내가 풀고 싶은 문제에 맞게 잘 정의하는 것이 중요



# 3. Gradient descent

* 일단 weight와 bias를 아무렇게 설정한 후, 현재 weight와 bias 위치에서 Loss를 줄이는 방향으로 학습 진행

* 모든 weight와 bias를 다 계산하여 Loss가 최소가 되는 weight와 bias를 찾는 방안은 비용이 큼

* Gradient는 한상 가파른 방향을 향함

* Loss를 weight와 bias로 편미분 하고, 현재의 weight와 bias 값을 대입함

* 두 편미분 값을 묶어 하나의 벡터를 만듦

  * 만들어진 벡터는 Loss가 가장 가파르게 증가하는 방향을 나타냄

  * Loss가 최소인 지점을 찾기 위해 Gradient의 반대 방향으로 weight와 bias를 이동

    ![Gradient descent algorithm explained with linear regression example | by  Dhanoop Karunakaran | Intro to Artificial Intelligence | Medium]({{site.url}}/images/2023-04-10-Chapter 3/1iIWg6s7FdhBbvPgDWG65mA-1681170465390-3.png)

  * weight와 bias를 Gradient의 반대 방향으로 이동시킬 때, learning rate 곱하여 이동시킴

    * $ x^2 + u^2 $의 경우를 예로 들어 생각했을 때, (1, 1) 지점에서 편미분하고 weight와 bias를 이동시키면 동일한 두 좌표를 반복하여 이동하는 것을 알 수 있음
    * 이는 최소 Loss값을 기준으로 반대로 이동한 것으로 생각하면 됨
    * 이런 경우를 해결하기 위해 0과 1 사이의 값인 learning rate를 곱하여 weight와 bias를 이동시킴
      * learning rate를 주어도 결국에는 Loss의 최소점을 기준으로 자꾸 반대로 이동하게 되는데, 이는 문제가 없음
      * Loss의 최소점에 도달하면 Gradient의 크기는 0에 수렴하기 때문
    * learning rate는 hyper parameter로 상수일 수 있고, 값을 변하게 할 수 있음
      * learning rate scheduling

* Gradient descent의 단점

  * 너무 신중하게 방향을 선택하는
    * 모든 데이터를 전부 고려하여 방향을 결정
    * 이를 해결하기 위해 SGD
  * Local Minimum에 빠질 위험이 있음
    * Global Minimum이 아닌 Local Minimum에 빠질 위험이 있음
    * 현재는 유의미한 Local Minimum을 찾을 수 있도록 학습이 진행되고 있음



# 4. 가중치 초기화 기법 정리

![img]({{site.url}}/images/2023-04-10-Chapter 3/images%2Fyh8109%2Fpost%2F73060deb-c718-4248-89eb-818e2a668b0d%2Fimage.png)

* Lecun initialization
  * uniform, normal distribution. 두 방식이 존재
  * 신경망에 들어오는 input의 크기가 커질수록 초기화 값의 분산을 작게하는 방법
  * 평균은 0이고 분산이 작아진다면, 결국 0에 가까운 수로 초기화 하는 방법 

![img](https://velog.velcdn.com/images%2Fyh8109%2Fpost%2F43403299-c0f1-44bf-b89a-fa9329a71d89%2Fimage.png)

* Xavier initialization
  * 신경망의 input, output 크기 모두를 고려하는 방법으로, input, output 크기가 커질수록 분산을 작게 하는 방법
  * Lecun 방식과 동일하게, 분산이 작아진다면, 결국 0에 가까운 수로 초기화 된다.
  * Sigmoid, Tanh와 같은 S자 형태의 활성화 함수에 적합한 초기화 방법

![img]({{site.url}}/images/2023-04-10-Chapter 3/images%2Fyh8109%2Fpost%2Fbaaf4d48-f14c-4c99-8053-420fff0a4dfa%2Fimage.png)

* He initialization
  * ReLU와 같은 활성화 함수에 적합한 초기화 방법
  * input 크기만 고려하여 가중치 값을 초기화하는 방법
  * input 크기가 커질수록 분산을 작게하여, 0에 가깝게 초기화



# 5. GD vs SGD(Stochastic Gradient descent)

* SGD
  * 랜덤하게 데이터 하나씩 뽑아서 Loss를 만듦(랜덤이라 stochastic)
  * 즉 하나의 데이터만 보고 빠르게 방향을 결정
    * 모든 데이터의 Loss 그래프를 등치선(contour)에서 바라보았을 때, SGD는 모든 데이터의 Loss 그래프의 Minimum 방향으로 학습이 되지 않는 것을 확인할 수 있다.
    * 모든 데이터의 Loss를 계산하는 것이 아닌 하나의 데이터의 Loss를 계산하기 때문
    * 대신 비복원 추출과 같이 다음에는 기존에 사용하지 않은 데이터를 활용하여 Loss를 계산하기 때문엔, 결국에는 모든 데이터를 사용한다.
  * GD 방식과 대비하여 빠르게 weight와 bias를 업데이트할 수 있다. 
  * SGD는 Local Minimum을 탈출할 수 있는 기회가 되기도 한다.



# 6. mini-batch SGD

* SGD는 하나의 데이터를 기반으로 가중치를 학습하니 너무 성급하게 방향 결정
* 그래서 나타난 방법이 mini-batch SGD로, 하나의 데이터가 아닌 두 개 이상의 데이터를 활용하는 방법
* mini-batch SGD의 크기
  * 무작정 batch size를 키운다고 좋은 것이 아니다.
  * 적정 batch size의 크기는 8000 정도로 나타남
    * https://arxiv.org/abs/1706.02677
  * batch size를 키우니 error 값도 커지는 결과가 나타남
  * 그 이유를 추측하건데, batch size를 키우면 GD와 동일한 효과를 가질 수 있는데, GD는 local minimum에 빠질 위험이 있음
  * batch size를 키우는 것 역시 local minimum에 빠질 위험이 커지는 것과 동일할 수 있음
* batch size를 키우면서 성능을 높히는 방안
  * batch size를 키우는 만큼 learning rate를 키우니 성능이 괜찮게 나타남
  * warmup 방식도 도입
    * learning rate를 초반에 목표치에 가깝게 되도록 조금씩 증가시킴
    * 목표치에 도달 후, learning rate를 유지하거나, 조금씩 감소시키는 방법



# 7. Moment vs RMSProp

![5) 옵티마이저 - 한땀한땀 딥러닝 컴퓨터 비전 백과사전]({{site.url}}/images/2023-04-10-Chapter 3/scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FMqP1e%2Fbtq7C3C3FYs%2FiWuVxYzNQ8wLU5IERtXXI0%2Fimg.jpg)

* Momentum
  * gradient의 문제점
    * gradient는 가장 가파른 방향을 향함
    * 이는 가장 평평한 부분에 수직인 방향임
    * "gradient는 contour(등치선)에 수직하다." 라는 내용은 수학적으로 증명 됨
    * 그래서 minimum 방향으로 향하는 것이 아닌 가장 가파른 방향으로 향함
    * 등치선이 더 심하게 찌그러져 있을수록 왼쪽 이미지와 같이 좌우로 심하게 움직이게 됨
  * Momentum은 기존 gradient에 관성을 부여한 방법론으로 좌우로 심하게 움직이는 현상을 완화해 줄 수 있음
    * gradient 값이 (앞, 좌), (앞, 우) ... 로 계산되었다고 한다면, 왼쪽 이미지와 같이 좌우로 심하게 움직이는 현상이 나타남
    * 그러나 (앞, 우)로 이동할 때, 기존 (앞, 좌)의 움직임을 고려하여 움직이게 되어 오른쪽의 이미지와 같이 좌우로 덜 움직이는 양상으로 됨
    * 다만, (앞, )이 누적되어 더해지기 때문에, minimum에 도착하면 contour 상에서 한바퀴 돌아가는 현상이 나타남(치타와 유사)
* RMSprop
  * 만약 weight, bias의 gradient 값이 $ \begin{matrix} 1 \\ 3 \end{matrix} $ 로 계산되었다면, gradient의 크기로 나눔
  * 수정된 크기의 gradient 만큼 weight, bias를 이동하여, 급격한 변화를 막음
  * leraning rate를 gradient 별로 다르게 준 효과와 동일
  * 가중치별 평준화하여 탐색한다는 개념



# 8. Adam

![The Adam optimizer - Deep Learning Quick Reference [Book]]({{site.url}}/images/2023-04-10-Chapter 3/898ba591-7dc4-4be2-8a81-3eed0141913b.png)

* RMSprop와 adam을 적절하게 섞은 방식
* exponential moving average

* $ \epsilon $ 의 역할

  * $ v_{t} $는 minimum에 근접하게 되어 0에 가까운 수로 됨

  * 이때, 만약 $ \epsilon $이 존재하지 않는다면, minimum에 가까워진 gradient에 의해 변화해야 하는 크기가 엄청 커질 수 있음

  * 단순히 분모가 0으로 나눠지는 현상을 해결해주는것 외에도 $ \epsilon $이 위의 문제를 해결해 줄 수 있음

    
