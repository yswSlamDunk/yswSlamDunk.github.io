---
layout : single
title : "[DeepLearningBasic] Chapter 1. 딥러닝을 위한 필수 기초 수학"
categories : [DeepLearning, DeepLearningBasic]
tag : [혁팬하임]
use_math: true
---

# 01. 함수와 다변수 함수
*  학생 때, 배운 함수의 정의는 하나의 입력은 하나의 출력이 대응된다고 배움
* 딥러닝을 수학적으로 표현하는 과정에서, 하나의 입력에 여러 개의 출력이 나타나는 경우 존재
* 이는 학생 때 배운 함수의 정의에 위배되는 것으로 보이나, 여러 개의 출력이 아닌 하나의 묶음이 출력되는 것으로 해석하면 됨
*  즉, 여러 개의 출력이 하나로 묶인 함수가 출력되는 것
* 이런 함수를 표현하는 방법은 이미지의 내용처럼 볼드체를 이용하고, 괄호를 통해 묶음을 표시한다.

![image-20230410104108816]({{site.url}}/images/2023-04-10-Chapter 1. 딥러닝을 위한 필수 기초 수학/image-20230410104108816.png)

* 원의 방정식이란 표현하고 원의 함수라고 표현하지 않음
*  그 이유는 함수는 하나의 입력은 하나의 출력이 대응되어야 하는데, 이를 만족하지 않기 때문



# 02. 로그 함수

1. $ \log_{a^m} x = \frac{1}{m} * \log_{a} x $

2. $  \log_{a} b = \frac{\log_{c} b} {\log_{c} a} $

3. $ \log_{e} x = \ln x $



# 03. 벡터와 행렬

* 여러 개의 연립 1차 방정식을 하나의 방정식으로 표현하기 위해 벡터와 행렬이 사용된다.

![image-20230410113003927]({{site.url}}/images/2023-04-10-Chapter 1. 딥러닝을 위한 필수 기초 수학/image-20230410113003927.png)

* norm은 벡터의 크기를 의미



# 04. 전치와 내적

#### 전치는 Transpose

* $ Ax = b $ A : 행렬, x,와 b : 벡터
* 위의 식의 Transpose를 구하면
  * $ (Ax)^{T} = (b)^{T} $
  * $ x^{T}A^{T} = b^{T} $

#### 내적

* 내적은 닮은 정도를 나타낸다

  ![image-20230410132831483]({{site.url}}/images/2023-04-10-Chapter 1. 딥러닝을 위한 필수 기초 수학/image-20230410132831483.png)

  * 위의 내용은 Transformer에서 사용된다.
  * $ \begin{Vmatrix} x \\ y \end{Vmatix}는 벡터의 크기를 나타냄



# 05. 극한과 입실론-델타 논법

* $ \lim_{n \to a}f(x) $ : x 가 a랑 무진장 가까운 값일 때, f(x)는 어떤 값과 무진장 가까워지는가?
* 만약 좌극한값과 우극한값이 다르다면, 극한값은 존재하지 않는다.
* 입실론은 매우 작은 양의 수로 인식하면 됨

![image-20230410134105279]({{site.url}}/images/2023-04-10-Chapter 1. 딥러닝을 위한 필수 기초 수학/image-20230410134105279.png)

![엡실론-델타 논법 - 위키백과, 우리 모두의 백과사전]({{site.url}}/images/2023-04-10-Chapter 1. 딥러닝을 위한 필수 기초 수학/220px-Límite_01.svg.png)

![엡실론-델타 논법]({{site.url}}/images/2023-04-10-Chapter 1. 딥러닝을 위한 필수 기초 수학/images.png)

* 위의 두 번째 이미지, 극한값이 존재하지 않는 경우를 살펴보면, 입실론 안으로 보낼 수 있는 델타가 존재하지 않는다. 



# 06. 미분과 도함수

![image-20230410135931080]({{site.url}}/images/2023-04-10-Chapter 1. 딥러닝을 위한 필수 기초 수학/image-20230410135931080.png)



# 07. 연쇄 법칙

* 딥러닝에서 가장 많이 사용되는 미분법칙

![image-20230410140551385]({{site.url}}/images/2023-04-10-Chapter 1. 딥러닝을 위한 필수 기초 수학/image-20230410140551385.png)



# 08. 편미분과 그라디언트

* 여러 개 변수로 이루어진 함수를 미분할 때 각각에 대해 미분하는 방법이 편미분
  * 여러 개의 변수를 입력으로 받는 함수
* 편미분의 결과는 변수별 미분 값이 벡터의 형태로 나타나며, 이를 그라디언트라고 함
* 미분 값을 구해야 하는 경우, 그라디언트에 해당 값을 대입하여 구하면 됨
* 편미분에 사용하는 기호는 $ \partial $ 를 사용



# 09. 테일러 급수

* 테일러 급수는 편미분의 결과 그라디언트가 가장 가파른 방향을 향하는지에 대해 설명할 때 사용됨
* 테일러 급수
  * 어떤 임의의 함수를 다항함수로 나타내는 것
  * 다항함수란? 다항식으로 표현된 함수
  * 다항식이란? 변수와 상수의 곱과 합으로 이루어진 식
  * 항이란? 변수와 상수의 곱으로 이루어진 것
* $ cos(x) $ 와 같이 다항식이 아닌 함수를 $ x, x^2, x^3 $ ... 이런 것들을 잘 조합해서 나타내기로 하는 것
  * 위의 "조합"한다는 것은 더하는 것을 의미, 이를 급수라고 함
* 왜 다항식으로 표현하는 것이 필요한가?
  * 다항식으로 표현하면 전구간 미분 가능, 미분도 간단해서 다루기 쉬움
* $ cos(x) $를 다항식으로 만드는 과정
  * 우선 0을 대입하고 미분을 하는 식으로 진행
  * 위의 방식을 계속 반복하면 각 항의 계수를 하나씩 파악할 수 있음
* 위의 방법으로 구한 다항식이 아닌 다른 다항식으로 나타낼 순 없는가? 
  * 없음
  * 각 항마다 하는 역할이 서로 독립이다.
* 위의 방법은 $ x = 0 $ 에서 성능이 좋다(잘 맞춘다). 그런데, 0에서 멀어질수록 성능이 안좋아진다. 이런 것을 해결할 수 있는 방법은 없는가?
  * 위의 급수는 테일러의 급수가 아닌, Maclaurin 급수임
  * 테일러의 급수는 특정 범위에서 성능이 잘 나타나도록 다항식을 만드는 방법

* 테일러의 급수

  * 어떤 함수를 다항식으로 표현하는 것은 동일하나 특정 기준(점)을 잡아줄 수 있음
  * 위의 Maclaurin 급수는 0을 대입했는데, 테일러의 급수에선 a를 대입함

* 테일러의 급수를 사용한다고 해서 모든 구간에서 다항함수로 표현할 수 있는 것은 아님

  * $ \ln x $ 의 예를 들면, x < 2 범위에서만 잘 표현한다(값이 올바르게 수렴한다).
  * $ \ln x $는 0 이상의 범위에 해당하는 함수인데, 전 구간에 해당하는 다항식의 조합으로 나타내려고 하니 적절하지 않아서 그럴 수 있다고 추측

* 수렴하는 구간을 만족하는 x의 구간은 아래의 식을 통해서 확인할 수 있음

  ![image-20230410151301131]({{site.url}}/images/2023-04-10-Chapter 1. 딥러닝을 위한 필수 기초 수학/image-20230410151301131.png)

  * 수렴하는 영역의 반지름은 radius of convergence라고 표현함



# 10. 스칼라를 벡터로 미분하는 법

